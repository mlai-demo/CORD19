{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 95%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 95%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json; print('JSON version:', json.__version__)\n",
    "import pandas as pd; print('Pandas version:', pd.__version__)\n",
    "import numpy as np; print('Numpy version:', np.__version__)\n",
    "from tqdm import tqdm; print('tqdm version:', pd.__version__)\n",
    "import nltk; print('NLTK version:', nltk.__version__)\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet') \n",
    "import string\n",
    "import re; print('Regex version:', re.__version__)\n",
    "import wordcloud; print('Wordcloud version:', wordcloud.__version__)\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import gensim; print('Gensim version:', gensim.__version__)\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import matplotlib; print('Matplotlib version:', matplotlib.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "file_dir = os.getcwd(); file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(file_dir + '\\metadata.readme', 'r') as fm:\n",
    "    data_meta = fm.read()\n",
    "    print(data_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(file_dir + \"/metadata.csv\", low_memory=False)\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "file_list = []\n",
    "for dirname, _, filenames in os.walk(file_dir):\n",
    "    for filename in filenames:\n",
    "        if filename[-5:]==\".json\":\n",
    "            file_list.append(os.path.join(dirname, filename))\n",
    "\n",
    "file_list.sort()\n",
    "total_files = len(file_list); total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts only - uncomment as needed to include other pieces\n",
    "\n",
    "start = time.time()\n",
    "docs = []\n",
    "# all_docs = []\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    j = json.load(open(file, \"rb\"))\n",
    "#     paper_id = j['paper_id']\n",
    "#     title = j['metadata']['title']\n",
    "       \n",
    "    abstract = \"\"\n",
    "    \n",
    "    try:\n",
    "        if j['abstract']:\n",
    "                for entry in j['abstract']:\n",
    "                    abstract += entry['text'] +'\\n\\n'\n",
    "    except KeyError:\n",
    "            pass \n",
    "            \n",
    "#     all_bodytext = \"\"\n",
    "        \n",
    "#     for text in j['body_text']:\n",
    "#         all_bodytext += text['text'] +'\\n\\n'\n",
    "\n",
    "#     all_docs.append([paper_id, title, abstract, all_bodytext])\n",
    "    docs.append([abstract])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_abstracts.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for words in docs:\n",
    "        f.write(\"%s\\n\" % words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "with open('all_abstracts.txt', encoding=\"utf-8\") as f, open('all_abstracts_tokens.txt', 'w', encoding=\"utf-8\") as out_f:\n",
    "    text = f.read().lower() #read and lower the case\n",
    "    short_words = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "    text = short_words.sub('', text) # get rid of short words (less than four letters long)\n",
    "    tokens = word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = stopwords.words('english')\n",
    "    new_stop_words = ['preprint', 'copyright', 'holder', 'peerreviewed', 'authorfunder', 'license', 'medrxiv', 'biorxiv',\n",
    "                     'righta', 'reuse', 'reserved', 'also', 'used', 'found', 'using', 'however']\n",
    "    stop_words.extend(new_stop_words) #enhance NLTK's list of stop words to be removed from the tokenized text\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    fd = nltk.FreqDist(words)\n",
    "    fd.plot(40,title = \"40 Most Frequent Words\", cumulative=False)    \n",
    "    out_f.write(new_text)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(words)\n",
    "freq_bigrams = nltk.FreqDist(bigrams)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "freq_bigrams.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_abstracts_tokens.txt', encoding = \"utf-8\") as f, open('all_abstracts_lemmas.txt', 'w', encoding = \"utf-8\") as out_f:\n",
    "    text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemmed = [lemma.lemmatize(word) for word in tokens]\n",
    "    new_lem_text = ' '.join(lemmed)\n",
    "    out_f.write(new_lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "lemma_text = open('all_abstracts_lemmas.txt', 'rt', encoding=\"utf-8\").read()\n",
    "\n",
    "wc = WordCloud(max_font_size=200,\n",
    "                      width=2500,\n",
    "                      height=2000,\n",
    "                      max_words=4000,\n",
    "                      random_state=44,\n",
    "                      collocations = False,\n",
    "                     ).generate(lemma_text)\n",
    "\n",
    "plt.figure(figsize=(32, 14))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"CORD abstracts\", fontsize= 20)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "n = 4\n",
    "word = r'\\W*([\\w]+)'\n",
    "text_search_como = re.findall(r'{}\\W*{}{}'.format(word*n,'(?:comorbid|comorbidity|comorbidities|comorbid |comorbidity |comorbidities | comorbid| comorbidity| comorbidities| comorbid | comorbidity | comorbidities )',word*n), lemma_text)\n",
    "# print(text_search)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_search_como = pd.DataFrame(text_search)\n",
    "df_text_search_como.to_csv('text_search_como.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_text_search = [element for sublist in text_search_como for element in sublist if len(element) >3]         \n",
    "# print(flatten_text_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flatten_text_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts = pd.DataFrame(Counter(flatten_text_search).most_common())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # show all sorted counts data\n",
    "    print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(24, 10), 'figure.dpi':300})\n",
    "\n",
    "counts = dict(Counter(flatten_text_search).most_common(150))\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# add labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(indexes + width * 0.05, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as abstracts-only above, just everything is uncommented\n",
    "\n",
    "start = time.time()\n",
    "all_docs = []\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    j = json.load(open(file, \"rb\"))\n",
    "    paper_id = j['paper_id']\n",
    "    title = j['metadata']['title']\n",
    "       \n",
    "    abstract = \"\"\n",
    "    \n",
    "    try:\n",
    "        if j['abstract']:\n",
    "                for entry in j['abstract']:\n",
    "                    abstract += entry['text'] +'\\n\\n'\n",
    "    except KeyError:\n",
    "            pass \n",
    "            \n",
    "    all_bodytext = \"\"\n",
    "        \n",
    "    for text in j['body_text']:\n",
    "        all_bodytext += text['text'] +'\\n\\n'\n",
    "\n",
    "    all_docs.append([paper_id, title, abstract, all_bodytext])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_docs.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for words in all_docs:\n",
    "        f.write(\"%s\\n\" % words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "with open('all_docs.txt', encoding=\"utf-8\") as f, open('all_docs_tokens.txt', 'w', encoding=\"utf-8\") as out_f:\n",
    "    text = f.read().lower()\n",
    "    short_words = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "    text = short_words.sub('', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = stopwords.words('english')\n",
    "    new_stop_words = ['preprint', 'copyright', 'holder', 'peerreviewed', 'authorfunder', 'license', 'medrxiv', 'biorxiv',\n",
    "                     'righta', 'reuse', 'reserved', 'also', 'used', 'found', 'using', 'however']\n",
    "    stop_words.extend(new_stop_words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    fd = nltk.FreqDist(words)\n",
    "    fd.plot(40,title = \"40 Most Frequent Words\", cumulative=False))\n",
    "    out_f.write(new_text)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(words)\n",
    "freq_bigrams = nltk.FreqDist(bigrams)\n",
    "plt.figure(figsize=(16, 7))\n",
    "freq_bigrams.plot(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_docs_tokens.txt', encoding = \"utf-8\") as f, open('all_docs_lemmas.txt', 'w', encoding = \"utf-8\") as out_f:\n",
    "    text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemmed = [lemma.lemmatize(word) for word in tokens]\n",
    "    new_lem_text = ' '.join(lemmed)\n",
    "    out_f.write(new_lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "lemma_text_all = open('all_docs_lemmas.txt', 'rt', encoding=\"utf-8\").read()\n",
    "\n",
    "wc = WordCloud(max_font_size=200,\n",
    "                      width=2500,\n",
    "                      height=2000,\n",
    "                      max_words=4000,\n",
    "                      random_state=44,\n",
    "                      collocations = False,\n",
    "                     ).generate(lemma_text_all)\n",
    "\n",
    "plt.figure(figsize=(32, 14))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"CORD documents\", fontsize= 20)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "word = r'\\W*([\\w]+)'\n",
    "text_search_all = re.findall(r'{}\\W*{}{}'.format(word*n,'(?:comorbid|comorbidity|comorbidities|comorbid |comorbidity |comorbidities | comorbid| comorbidity| comorbidities| comorbid | comorbidity | comorbidities )',word*n), lemma_text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_text_search_all = [element for sublist in text_search_all for element in sublist if len(element) >3] \n",
    "sorted_counts = pd.DataFrame(Counter(flatten_text_search_all).most_common())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # show all sorted counts data\n",
    "    print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(24, 10), 'figure.dpi':300})\n",
    "\n",
    "counts = dict(Counter(flatten_text_search_all).most_common(150))\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# add labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(indexes + width * 0.05, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_docs = pd.DataFrame(all_docs, columns=['paper_id', 'title', 'abstract', 'all_bodytext'])\n",
    "print(df_all_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = df_all_docs[(df_all_docs['abstract'].str.contains('comorbid')) | (df_all_docs['abstract'].str.contains('Comorbid')) | (df_all_docs['abstract'].str.contains('comorbidity')) | (df_all_docs['abstract'].str.contains('Comorbidity')) | (df_all_docs['abstract'].str.contains('comorbidities')) | (df_all_docs['abstract'].str.contains('Comorbidities'))\n",
    "                  | (df_all_docs['all_bodytext'].str.contains('comorbid')) | (df_all_docs['all_bodytext'].str.contains('Comorbid')) | (df_all_docs['all_bodytext'].str.contains('comorbidity')) | (df_all_docs['all_bodytext'].str.contains('Comorbidity')) | (df_all_docs['all_bodytext'].str.contains('comorbidities')) | (df_all_docs['all_bodytext'].str.contains('Comorbidities'))]\n",
    "\n",
    "df_risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_searched = df_risk['abstract'].values\n",
    "bodytext_searched = df_risk['all_bodytext'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_risk_sentences = pd.DataFrame([])\n",
    "\n",
    "for s in tqdm(abstract_searched):\n",
    "    for sentence in s.split('. '):\n",
    "        if \"comorbid\" in sentence:\n",
    "            risk_sentences = pd.DataFrame([sentence])\n",
    "            df_risk_sentences  = df_risk_sentences.append(risk_sentences)\n",
    "#             df_risk_sentences.to_csv(\"df_risk_sentences.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None):  # show all risk sentences\n",
    "    print(df_risk_sentences)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_risk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = df_risk[(df_risk['abstract'].str.contains('It has been noted that elderly patients'))]\n",
    "with pd.option_context('display.max_rows', None):  \n",
    "    print(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"179df1e769292dd113cef1b54b0b43213e6b5c97.json\"\n",
    "\n",
    "counter = 0\n",
    "file_select = []\n",
    "for dirname, _, filenames in os.walk(file_dir):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        if filename==paper_id:\n",
    "            file_select.append(os.path.join(dirname, filename))\n",
    "\n",
    "file_select = ''.join(file_select) #convert to string\n",
    "\n",
    "with open(file_select) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for excerpt in json_data['abstract']:\n",
    "    with pd.option_context('display.max_rows', None):  # show all risk sentences\n",
    "        print(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk_sentences_bodytext = pd.DataFrame([])\n",
    "\n",
    "for s in tqdm(bodytext_searched):\n",
    "    for sentence in s.split('. '):\n",
    "        if \"comorbid\" in sentence:\n",
    "            risk_sentences = pd.DataFrame([sentence])\n",
    "            df_risk_sentences_bodytext  = df_risk_sentences_bodytext.append(risk_sentences)\n",
    "#             df_risk_sentences_bodytext.to_csv(\"df_risk_sentences_alldocs.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None):  # show all risk sentences\n",
    "    print(df_risk_sentences_bodytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_risk_sentences_bodytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_risk_sentences_bodytext = pd.DataFrame([])\n",
    "\n",
    "for s in tqdm(bodytext_searched):\n",
    "    for sentence in s.split('. '):\n",
    "        if \"risk factor\" in sentence:\n",
    "            risk_sentences = pd.DataFrame([sentence])\n",
    "            df_risk_sentences_bodytext  = df_risk_sentences_bodytext.append(risk_sentences)\n",
    "            df_risk_sentences_bodytext.to_csv(\"df_risk_sentences_alldocs.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None):  \n",
    "    print(df_risk_sentences_bodytext)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Processed in %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term_bodytext = df_risk[(df_risk['all_bodytext'].str.contains('The association between comorbidities and ALI'))]\n",
    "with pd.option_context('display.max_rows', None):  \n",
    "    print(search_term_bodytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"061ffcdd4d674c4d7ce24e4aa7c5037c68596864.json\"\n",
    "\n",
    "counter = 0\n",
    "file_select = []\n",
    "for dirname, _, filenames in os.walk(file_dir):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        if filename==paper_id:\n",
    "            file_select.append(os.path.join(dirname, filename))\n",
    "\n",
    "file_select = ''.join(file_select) #convert to string\n",
    "\n",
    "with open(file_select) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for excerpt in json_data['body_text']:\n",
    "    with pd.option_context('display.max_rows', None):  \n",
    "        print(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = open('kaggle/working/all_docs.txt', 'r', encoding = 'utf-8') \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in tqdm(sent_tokenize(f)): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words_skipgram = {search_term: [item for item in model_skipgram.wv.most_similar([search_term], topn=300)]\n",
    "                  for search_term in ['comorbidity']}\n",
    "similar_words_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity between 'comorbidity' \" + \"and 'asthma' - SG : \", \n",
    "    round(model_skipgram.wv.similarity('comorbidity', 'asthma'),2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (covid19)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
